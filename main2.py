import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import models
import wandb
from dataset_double import DoubleSkinDataset #, AugmentedSkinDataset
from model import Model
import argparse
from datetime import datetime
import numpy as np
import random
from sklearn.model_selection import StratifiedShuffleSplit
import glob


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dirs', nargs='+', required=True, help='ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ê²½ë¡œë“¤ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)')
    parser.add_argument('--output_dir', type=str, default='checkpoints', help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--epochs', type=int, default=100, help='í•™ìŠµ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--lr', type=float, default=0.0005, help='ì´ˆê¸° í•™ìŠµë¥ ')
    parser.add_argument('--num_workers', type=int, default=4, help='ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜')
    parser.add_argument('--resume', type=str, default=None, help='ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    
    # wandb ê´€ë ¨ íŒŒë¼ë¯¸í„°
    parser.add_argument('--wandb_project', type=str, default='skin-classifier', help='wandb í”„ë¡œì íŠ¸ ì´ë¦„')
    parser.add_argument('--wandb_name', type=str, default=None, help='wandb ì‹¤í—˜ ì´ë¦„')
    parser.add_argument('--wandb_dir', type=str, default=None, help='wandb ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: í˜„ì¬ ë””ë ‰í† ë¦¬/wandb_logs)')
    parser.add_argument('--wandb_id', type=str, default=None, help='wandb run id (resume ì‹œ ì‚¬ìš©)')
    parser.add_argument('--wandb_resume', type=str, default='never', choices=['never', 'allow', 'must'], help='wandb resume ì˜µì…˜ (never/allow/must)')
    
    
    # ê·œì œ ê´€ë ¨ íŒŒë¼ë¯¸í„°
    parser.add_argument('--weight_decay', type=float, default=1e-3, help='ê°€ì¤‘ì¹˜ ê°ì†Œ (L2 ê·œì œ) ê°•ë„') # 1e-7, 1e-4 ~ 1e-3
    parser.add_argument('--dropout_prob', type=float, default=0.3, help='ë“œë¡­ì•„ì›ƒ í™•ë¥ ') # 0.2, 0.3 ~ 0.4
    
    # ì†ì‹¤ í•¨ìˆ˜ ê´€ë ¨ íŒŒë¼ë¯¸í„°
    parser.add_argument('--train_loss', type=str, default='focal', 
                       choices=['focal', 'cb', 'label_smoothing', 'cross_entropy'],
                       help='í•™ìŠµì— ì‚¬ìš©í•  ì†ì‹¤ í•¨ìˆ˜')
    parser.add_argument('--gamma', type=float, default=2.0, help='Focal Lossì™€ CB Lossì˜ gamma ê°’')
    parser.add_argument('--beta', type=float, default=0.999, help='CB Lossì˜ beta ê°’')
    parser.add_argument('--alpha', type=float, default=1.0, help='Focal Lossì˜ alpha ê°’')
    parser.add_argument('--smoothing', type=float, default=0.1, help='Label Smoothingì˜ smoothing ê°’')
    
    # ë°ì´í„° ì¦ê°• ê´€ë ¨ íŒŒë¼ë¯¸í„°
    parser.add_argument('--use_augmentation', action='store_true', help='ë°ì´í„° ì¦ê°• ì‚¬ìš© ì—¬ë¶€')
    parser.add_argument('--target_samples_per_class', type=int, default=None, 
                       help='í´ë˜ìŠ¤ë³„ ëª©í‘œ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¡œ ì„¤ì •)')
    parser.add_argument('--augmentation_methods', nargs='+', 
                       default=['horizontal_flip', 'rotation', 'color_jitter'],
                       choices=['horizontal_flip', 'rotation', 'color_jitter'],
                       help='ì‚¬ìš©í•  ì¦ê°• ë°©ë²•ë“¤')
    
    # ì‹¤í—˜ ì¬í˜„ì„± ê´€ë ¨ íŒŒë¼ë¯¸í„°
    parser.add_argument('--reproducible', action='store_true', default=True,
                       help='ì‹¤í—˜ ì¬í˜„ì„± ë³´ì¥ (ê¸°ë³¸ê°’: True)')
    parser.add_argument('--no_reproducible', action='store_true', default=False,
                       help='ì‹¤í—˜ ì¬í˜„ì„± ë¹„í™œì„±í™” (ì™„ì „ ëœë¤)')
    
    return parser.parse_args()

def set_seed(seed):
    """ëœë¤ ì‹œë“œ ì„¤ì • - ì‹¤í—˜ ì¬í˜„ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ ëª¨ë“  ëœë¤ ìš”ì†Œ ì œì–´"""
    # Python í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëœë¤ ì‹œë“œ ì„¤ì •
    random.seed(seed)
    # NumPy ëœë¤ ì‹œë“œ ì„¤ì • (ë°ì´í„°ì…‹ ë¶„í• , ìƒ˜í”Œë§ ë“±ì— ì‚¬ìš©)
    np.random.seed(seed)
    # PyTorch CPU ì—°ì‚° ëœë¤ ì‹œë“œ ì„¤ì •
    torch.manual_seed(seed)
    
    # cuDNN ì¬í˜„ì„± ì„¤ì •
    # deterministic=True: cuDNN ì—°ì‚°ì„ ê²°ì •ì ìœ¼ë¡œ ë§Œë“¤ì–´ ê°™ì€ ì…ë ¥ì— ëŒ€í•´ í•­ìƒ ê°™ì€ ê²°ê³¼ ë³´ì¥
    # benchmark=False: cuDNNì´ ìë™ìœ¼ë¡œ ìµœì  ì•Œê³ ë¦¬ì¦˜ì„ ì°¾ëŠ” ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ì—¬ ê³ ì •ëœ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
    # ì´ ë‘ ì„¤ì •ì€ ì„±ëŠ¥ë³´ë‹¤ ì¬í˜„ì„±ì„ ìš°ì„ ì‹œí•˜ëŠ” ì„¤ì • (ì—°êµ¬/ì‹¤í—˜ì—ì„œ í•„ìˆ˜)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # CUDA GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° GPU ëœë¤ ì‹œë“œ ì„¤ì •
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)      # í˜„ì¬ GPU ëœë¤ ì‹œë“œ
        torch.cuda.manual_seed_all(seed)  # ëª¨ë“  GPU ëœë¤ ì‹œë“œ (ë©€í‹° GPU í™˜ê²½)
    
    # Apple Silicon MPSê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° MPS ëœë¤ ì‹œë“œ ì„¤ì •
    if torch.backends.mps.is_available():
        torch.mps.manual_seed(seed)

def stratified_split_dataset(dataset, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, random_state=42):
    """í´ë˜ìŠ¤ ê· í˜•ì„ ê³ ë ¤í•œ ë°ì´í„°ì…‹ ë¶„í• """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "ë¹„ìœ¨ì˜ í•©ì´ 1ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    labels = [dataset.labels[i] for i in range(len(dataset))]
    labels = np.array(labels)
    
    # ë¨¼ì € trainê³¼ ë‚˜ë¨¸ì§€ë¡œ ë¶„í• 
    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=(val_ratio + test_ratio), random_state=random_state)
    train_idx, temp_idx = next(sss1.split(range(len(dataset)), labels))
    
    # ë‚˜ë¨¸ì§€ë¥¼ valê³¼ testë¡œ ë¶„í• 
    temp_labels = labels[temp_idx]
    val_test_ratio = val_ratio / (val_ratio + test_ratio)
    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=(1 - val_test_ratio), random_state=random_state)
    val_idx, test_idx = next(sss2.split(range(len(temp_idx)), temp_labels))
    
    # ì¸ë±ìŠ¤ ë§¤í•‘
    val_idx = temp_idx[val_idx]
    test_idx = temp_idx[test_idx]
    
    train_dataset = Subset(dataset, train_idx)
    val_dataset = Subset(dataset, val_idx)
    test_dataset = Subset(dataset, test_idx)
    
    return train_dataset, val_dataset, test_dataset

def get_device():
    """ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì„¤ì •"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    elif torch.backends.mps.is_available():
        # MPSì—ì„œ ì¼ë¶€ ì—°ì‚°ì´ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ CPU fallback ê³ ë ¤
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ MPS í˜¸í™˜ì„± í™•ì¸
            test_tensor = torch.randn(2, 2, device='mps')
            _ = torch.softmax(test_tensor, dim=1)
            return torch.device('mps')
        except:
            print("MPS ë””ë°”ì´ìŠ¤ì—ì„œ ì¼ë¶€ ì—°ì‚°ì´ ì§€ì›ë˜ì§€ ì•Šì•„ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return torch.device('cpu')
    else:
        return torch.device('cpu')

def main():
    args = parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = get_device()
    print(f"Using device: {device}")
    
    # ëœë¤ ì‹œë“œ ì„¤ì •
    if args.no_reproducible:
        print("ì‹¤í—˜ ì¬í˜„ì„± ë¹„í™œì„±í™”: ì™„ì „ ëœë¤ ëª¨ë“œ")
        # ëœë¤ ì‹œë“œ ì„¤ì •í•˜ì§€ ì•ŠìŒ
    else:
        print("ì‹¤í—˜ ì¬í˜„ì„± ë³´ì¥: ê³ ì •ëœ ëœë¤ ì‹œë“œ ì‚¬ìš©")
        set_seed(42)
    
    # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ë””ë ‰í† ë¦¬ ìƒì„±
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    args.output_dir = f"{args.output_dir}_{current_time}"
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ì‹¤í—˜ ID ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì¬ì‹œì‘ ì‹œì—ë„ ë™ì¼í•œ ID ì‚¬ìš©)
    experiment_id = current_time
    if args.resume:
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘í•˜ëŠ” ê²½ìš°, ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…ì—ì„œ ì‹¤í—˜ ID ì¶”ì¶œ
        try:
            checkpoint_dir = os.path.dirname(args.resume)
            if os.path.basename(checkpoint_dir).startswith('checkpoints_'):
                experiment_id = os.path.basename(checkpoint_dir).replace('checkpoints_', '')
        except:
            pass  # ì‹¤íŒ¨í•˜ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
    
    # wandb ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
    if args.wandb_dir:
        wandb_dir = args.wandb_dir
    else:
        wandb_dir = os.path.join(os.getcwd(), 'wandb')
    
    os.environ['WANDB_DIR'] = wandb_dir
    os.makedirs(wandb_dir, exist_ok=True)
    print(f"wandb ë¡œê·¸ ë””ë ‰í† ë¦¬: {wandb_dir}")
    
    # wandb ì´ˆê¸°í™”
    try:
        wandb_config = {
            "learning_rate": args.lr,
            "batch_size": args.batch_size,
            "epochs": args.epochs,
            "architecture": "ResNet50",
            "weight_decay": args.weight_decay,
            "dropout_prob": args.dropout_prob,
            "gamma": args.gamma,
            "use_augmentation": args.use_augmentation
        }
        
        if args.use_augmentation:
            wandb_config.update({
                "augmentation_methods": args.augmentation_methods,
                "target_samples_per_class": args.target_samples_per_class
            })
        
        wandb_config.update({
            "reproducible": not args.no_reproducible
        })
        
        wandb.init(
            project=args.wandb_project,
            name=args.wandb_name,
            config=wandb_config,
            id=args.wandb_id,
            resume=args.wandb_resume,
            dir=wandb_dir
        )
        use_wandb = True
        print("wandb ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        print(f"wandb ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("wandb ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        use_wandb = False
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = DoubleSkinDataset(args.data_dirs)
    
    # í´ë˜ìŠ¤ ê· í˜•ì„ ê³ ë ¤í•œ ë°ì´í„°ì…‹ ë¶„í• 
    train_dataset, val_dataset, test_dataset = stratified_split_dataset(
        dataset, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, random_state=42
    )
    
    # train ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚° í•¨ìˆ˜
    def get_train_class_counts(dataset, num_classes):
        counts = [0] * num_classes
        # Subsetì¸ ê²½ìš°
        if hasattr(dataset, 'dataset') and hasattr(dataset, 'indices'):
            for idx in dataset.indices:
                label = dataset.dataset.labels[idx]
                counts[label] += 1
        # AugmentedSkinDatasetì¸ ê²½ìš°
        elif hasattr(dataset, 'labels'):
            for label in dataset.labels:
                counts[label] += 1
        else:
            for i in range(len(dataset)):
                _, label, _ = dataset[i]
                counts[label] += 1
        return counts

    num_classes = len(dataset.classes)
    train_samples_per_cls = get_train_class_counts(train_dataset, num_classes)

    # Train ë°ì´í„°ì…‹ì—ë§Œ ì¦ê°• ì ìš© (ì˜µì…˜)
    if args.use_augmentation:
        print("\n=== ë°ì´í„° ì¦ê°• ì ìš© ===")
        print(f"ì¦ê°• ë°©ë²•: {', '.join(args.augmentation_methods)}")
        if args.target_samples_per_class:
            print(f"ëª©í‘œ ìƒ˜í”Œ ìˆ˜: {args.target_samples_per_class}ê°œ")
        else:
            print("ëª©í‘œ ìƒ˜í”Œ ìˆ˜: ìë™ ì„¤ì • (ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ ê¸°ì¤€)")
        
        # ì¦ê°•ëœ train ë°ì´í„°ì…‹ ìƒì„± (train ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©)
        augmented_train_dataset = AugmentedSkinDataset(
            root_dirs=args.data_dirs,
            transform=dataset.transform,
            target_samples_per_class=args.target_samples_per_class,
            augmentation_methods=args.augmentation_methods,
            train_indices=train_dataset.indices  # train ì¸ë±ìŠ¤ë§Œ ì „ë‹¬
        )
        train_dataset = augmented_train_dataset
        # ì¦ê°•ëœ ë°ì´í„°ì…‹ì˜ ë¶„í¬ë¡œ ë‹¤ì‹œ ê³„ì‚°
        train_samples_per_cls = get_train_class_counts(train_dataset, num_classes)
        print("ì¦ê°•ëœ train ë°ì´í„°ì…‹ìœ¼ë¡œ êµì²´ ì™„ë£Œ")
    else:
        print("\n=== ë°ì´í„° ì¦ê°• ë¯¸ì‚¬ìš© ===")
        print("ì›ë³¸ ë°ì´í„°ì…‹ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
    
    print(f"ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
    print(f"  í•™ìŠµ: {len(train_dataset)}ê°œ")
    print(f"  ê²€ì¦: {len(val_dataset)}ê°œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_dataset)}ê°œ")
    
    # í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
    def get_class_distribution(dataset):
        class_counts = {}
        
        # AugmentedSkinDatasetì¸ ê²½ìš°
        if hasattr(dataset, 'labels') and hasattr(dataset, 'classes'):
            # ì§ì ‘ labelsì™€ classes ì†ì„±ì— ì ‘ê·¼
            for idx in range(len(dataset)):
                label = dataset.labels[idx]
                class_name = dataset.classes[label]
                class_counts[class_name] = class_counts.get(class_name, 0) + 1
        # Subsetì¸ ê²½ìš° (ì›ë³¸ ë°ì´í„°ì…‹ì´ ê°ì‹¸ì ¸ ìˆìŒ)
        elif hasattr(dataset, 'dataset') and hasattr(dataset, 'indices'):
            # ì›ë³¸ ë°ì´í„°ì…‹ì˜ labelsì™€ classesì— ì ‘ê·¼
            for idx in range(len(dataset)):
                label = dataset.dataset.labels[dataset.indices[idx]]
                class_name = dataset.dataset.classes[label]
                class_counts[class_name] = class_counts.get(class_name, 0) + 1
        else:
            # ê¸°íƒ€ ê²½ìš°: ì§ì ‘ ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ì—¬ ë ˆì´ë¸” ìˆ˜ì§‘
            for idx in range(len(dataset)):
                try:
                    _, label, _ = dataset[idx]
                    # labelì´ ìˆ«ìì¸ ê²½ìš° í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë³€í™˜
                    if isinstance(label, int):
                        # ì›ë³¸ ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ì •ë³´ ì‚¬ìš©
                        class_name = dataset.classes[label] if hasattr(dataset, 'classes') else f"class_{label}"
                    else:
                        class_name = str(label)
                    class_counts[class_name] = class_counts.get(class_name, 0) + 1
                except Exception as e:
                    print(f"ë ˆì´ë¸” ì¶”ì¶œ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {idx}): {e}")
                    continue
        
        return class_counts
    
    print("\ní´ë˜ìŠ¤ë³„ ë¶„í¬:")
    print("  í•™ìŠµ:", get_class_distribution(train_dataset))
    print("  ê²€ì¦:", get_class_distribution(val_dataset))
    print("  í…ŒìŠ¤íŠ¸:", get_class_distribution(test_dataset))
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    # MPS ë””ë°”ì´ìŠ¤ì—ì„œëŠ” num_workersë¥¼ 0ìœ¼ë¡œ ì„¤ì • (í˜¸í™˜ì„± ë¬¸ì œ)
    safe_num_workers = 0 if device.type == 'mps' else args.num_workers
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, 
                            shuffle=True, num_workers=safe_num_workers)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size,
                          shuffle=False, num_workers=safe_num_workers)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size,
                           shuffle=False, num_workers=safe_num_workers)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = Model(num_classes, dropout_prob=args.dropout_prob, 
                 samples_per_cls=train_samples_per_cls).to(device)
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    # wandbì— ëª¨ë¸ êµ¬ì¡° ê¸°ë¡
    if use_wandb:
        wandb.watch(model)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    start_epoch = 0
    best_acc = 0
    best_epoch = -1  # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ì €ì¥ëœ ì—í­ ì¶”ì 
    
    if args.resume:
        try:
            model, optimizer, start_epoch, best_acc = model.load_checkpoint(args.resume, optimizer, scheduler, device)
            print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: ì—í¬í¬ {start_epoch}ë¶€í„° ì‹œì‘")
        except Exception as e:
            print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # í•™ìŠµ ë£¨í”„
    for epoch in range(start_epoch, args.epochs):
        # í•™ìŠµ
        train_loss, train_acc = model.train_epoch(
            train_loader, optimizer, device,
            loss_type=args.train_loss,
            gamma=args.gamma if args.train_loss in ['focal', 'cb'] else None,
            beta=args.beta if args.train_loss == 'cb' else None,
            alpha=args.alpha if args.train_loss == 'focal' else None,
            smoothing=args.smoothing if args.train_loss == 'label_smoothing' else None,
            epoch=epoch if not args.no_reproducible else None
        )
        
        # ê²€ì¦
        val_loss, val_acc, val_bal_acc, val_macro_f1 = model.validate(
            val_loader, device, log_dir=args.output_dir, epoch=epoch, experiment_id=experiment_id
        )
        
        # wandbì— ê¸°ë¡
        if use_wandb:
            wandb.log({
                "epoch": epoch,
                "train_loss": train_loss,
                "train_acc": train_acc,
                "val_loss": val_loss,
                "val_acc": val_acc,
                "val_balanced_acc": val_bal_acc,
                "val_macro_f1": val_macro_f1,
                "learning_rate": optimizer.param_groups[0]['lr']
            })
        
        print(f'Epoch: {epoch} | '
              f'Train Loss: {train_loss:.4f} | '
              f'Train Acc: {train_acc:.4f} | '
              f'Val Loss: {val_loss:.4f} | '
              f'Val Acc: {val_acc:.4f} | '
              f'Val Balanced Acc: {val_bal_acc:.4f} | '
              f'Val Macro F1: {val_macro_f1:.4f}')
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (val_balanced_acc ê¸°ì¤€)
        if val_bal_acc > best_acc:
            best_acc = val_bal_acc
            best_epoch = epoch  # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ì €ì¥ëœ ì—í­ ê¸°ë¡
            best_model_path = os.path.join(args.output_dir, f'best_model_epoch_{epoch}.pth')
            print(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±! ì—í¬í¬ {epoch}ì—ì„œ {os.path.basename(best_model_path)} ì €ì¥ (ê²€ì¦ Balanced Accuracy: {val_bal_acc:.4f})")
            model.save_checkpoint(optimizer, epoch, best_acc, best_model_path, scheduler)

            # ìµœê·¼ 3ê°œë§Œ ë‚¨ê¸°ê³  ì´ì „ ëª¨ë¸ ì‚­ì œ
            model_files = sorted(
                glob.glob(os.path.join(args.output_dir, "best_model_epoch_*.pth")),
                key=lambda x: int(x.split("_")[-1].split(".")[0])
            )
            if len(model_files) > 3:
                for old_file in model_files[:-3]:
                    try:
                        os.remove(old_file)
                        print(f"ğŸ—‘ï¸ ì´ì „ ëª¨ë¸ ì‚­ì œ: {os.path.basename(old_file)}")
                    except Exception as e:
                        print(f"âš ï¸ ëª¨ë¸ ì‚­ì œ ì‹¤íŒ¨: {old_file} ({e})")
            
        # ë§¤ ì—í¬í¬ ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
    
    # ìµœì¢… í…ŒìŠ¤íŠ¸
    print("\nìµœì¢… í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    test_loss, test_acc, test_bal_acc, test_macro_f1 = model.test(test_loader, device, args.output_dir, experiment_id)
    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f} | Test Balanced Acc: {test_bal_acc:.4f} | Test Macro F1: {test_macro_f1:.4f}')
    
    # í•™ìŠµ ì™„ë£Œ ìš”ì•½
    print(f"\nğŸ“Š í•™ìŠµ ì™„ë£Œ ìš”ì•½:")
    if best_epoch >= 0:
        best_model_path = os.path.join(args.output_dir, f'best_model_epoch_{best_epoch}.pth')
        print(f"   - ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì—í¬í¬: {best_epoch}")
        print(f"   - ìµœê³  ê²€ì¦ ì •í™•ë„: {best_acc:.4f}")
        print(f"   - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
        print(f"   - ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {best_model_path}")
    else:
        print(f"   - ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì—í¬í¬: ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ")
        print(f"   - ìµœê³  ê²€ì¦ ì •í™•ë„: {best_acc:.4f}")
        print(f"   - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
        print(f"   - ëª¨ë¸ ì €ì¥ ê²½ë¡œ: ì—†ìŒ")
    
    # wandbì— í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
    if use_wandb:
        wandb.log({
            "test_loss": test_loss,
            "test_acc": test_acc,
            "test_balanced_acc": test_bal_acc,
            "test_macro_f1": test_macro_f1,
        })
    
    if use_wandb:
        wandb.finish()

if __name__ == '__main__':
    main() 